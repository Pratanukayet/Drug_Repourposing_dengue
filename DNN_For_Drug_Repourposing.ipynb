{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1963f9-45c3-4bbe-80ed-c33f6c4c3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, T5Tokenizer, T5EncoderModel\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv1D, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras import layers, Model, Input, regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "# Set working directory\n",
    "working_dir = \"DL-M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffdef5-9f3c-44d8-a2c5-fc20feaa7898",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_file = os.path.join(working_dir, \"Drugs.csv\")\n",
    "chemberta_output = os.path.join(working_dir, \"ChemBERTa_drug_embeddings.csv\")\n",
    "morgan_output = os.path.join(working_dir, \"Morgan_fingerprints.csv\")\n",
    "\n",
    "# Load drug data\n",
    "drugs_df = pd.read_csv(drug_file)\n",
    "\n",
    "# Initialize ChemBERTa model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer_chemberta = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model_chemberta = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\").to(device)\n",
    "\n",
    "# Sliding window tokenization\n",
    "def tokenize_smiles(smiles, max_length=512, stride=256):\n",
    "    tokens = tokenizer_chemberta(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    if input_ids.shape[1] > max_length:\n",
    "        chunks = [input_ids[:, i:i+max_length] for i in range(0, input_ids.shape[1], stride)]\n",
    "        return chunks\n",
    "    return [input_ids]\n",
    "\n",
    "# Extract ChemBERTa embeddings\n",
    "def get_chemberta_embedding(smiles):\n",
    "    tokens_list = tokenize_smiles(smiles)\n",
    "    embeddings = []\n",
    "    for tokens in tokens_list:\n",
    "        tokens = tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_chemberta(input_ids=tokens)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n",
    "    return np.mean(np.vstack(embeddings), axis=0)\n",
    "\n",
    "# Compute Morgan fingerprints\n",
    "def get_morgan_fingerprint(smiles, radius=2, n_bits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.zeros(n_bits)\n",
    "    return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits))\n",
    "\n",
    "# Process drugs\n",
    "drug_embeddings = []\n",
    "drug_fingerprints = []\n",
    "\n",
    "for _, row in tqdm(drugs_df.iterrows(), total=len(drugs_df)):\n",
    "    smiles = row['Ligand_SMILES']\n",
    "    drug_id = row['DrugBank_ID']\n",
    "    \n",
    "    chemberta_emb = get_chemberta_embedding(smiles)\n",
    "    morgan_fp = get_morgan_fingerprint(smiles)\n",
    "    \n",
    "    drug_embeddings.append([drug_id] + chemberta_emb.tolist())\n",
    "    drug_fingerprints.append([drug_id] + morgan_fp.tolist())\n",
    "\n",
    "# Convert to DataFrames\n",
    "drug_embeddings_df = pd.DataFrame(drug_embeddings, columns=['DrugBank_ID'] + [f\"embedding_dim_{i+1}\" for i in range(chemberta_emb.shape[0])])\n",
    "drug_fingerprints_df = pd.DataFrame(drug_fingerprints, columns=['DrugBank_ID'] + [f\"fingerprint_{i+1}\" for i in range(morgan_fp.shape[0])])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "drug_embeddings_df.iloc[:, 1:] = scaler.fit_transform(drug_embeddings_df.iloc[:, 1:])\n",
    "drug_fingerprints_df.iloc[:, 1:] = scaler.fit_transform(drug_fingerprints_df.iloc[:, 1:])\n",
    "\n",
    "# Save feature files\n",
    "drug_embeddings_df.to_csv(chemberta_output, index=False)\n",
    "drug_fingerprints_df.to_csv(morgan_output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be4f2d-f043-4a4d-b585-d5e9a8bee0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_file = os.path.join(working_dir, \"Proteins.csv\")\n",
    "protT5_output = os.path.join(working_dir, \"ProtT5_protein_embeddings.csv\")\n",
    "cnn_output = os.path.join(working_dir, \"CNN_protein_embeddings.csv\")\n",
    "\n",
    "# Load protein data\n",
    "proteins_df = pd.read_csv(protein_file)\n",
    "\n",
    "# Initialize ProtT5 model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer_protT5 = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "model_protT5 = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\").to(device)\n",
    "\n",
    "# Sliding window tokenization\n",
    "def tokenize_sequence(sequence, max_length=512, stride=256):\n",
    "    tokens = tokenizer_protT5(sequence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    if input_ids.shape[1] > max_length:\n",
    "        chunks = [input_ids[:, i:i+max_length] for i in range(0, input_ids.shape[1], stride)]\n",
    "        return chunks\n",
    "    return [input_ids]\n",
    "\n",
    "# Extract ProtT5 embeddings\n",
    "def get_protT5_embedding(sequence):\n",
    "    tokens_list = tokenize_sequence(sequence)\n",
    "    embeddings = []\n",
    "    for tokens in tokens_list:\n",
    "        tokens = tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_protT5(input_ids=tokens)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).cpu().numpy())\n",
    "    return np.mean(np.vstack(embeddings), axis=0)\n",
    "\n",
    "# CNN Model for Non-Transformer Embeddings\n",
    "def build_cnn_model(input_length=1000, embedding_dim=128):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=3, activation='relu', input_shape=(input_length, 1)),\n",
    "        GlobalAveragePooling1D(),\n",
    "        Dense(embedding_dim, activation='relu')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "cnn_model = build_cnn_model()\n",
    "\n",
    "# Process proteins\n",
    "protein_embeddings = []\n",
    "cnn_embeddings = []\n",
    "\n",
    "for _, row in tqdm(proteins_df.iterrows(), total=len(proteins_df)):\n",
    "    sequence = row['Sequence']\n",
    "    protein_id = row['UniProt_ID']\n",
    "    \n",
    "    protT5_emb = get_protT5_embedding(sequence)\n",
    "    sequence_array = np.array([ord(aa) for aa in sequence])[:1000]  # Convert AA to integer representation\n",
    "    sequence_array = sequence_array.reshape(1, -1, 1)\n",
    "    cnn_emb = cnn_model.predict(sequence_array, verbose=0).flatten()\n",
    "    \n",
    "    protein_embeddings.append([protein_id] + protT5_emb.tolist())\n",
    "    cnn_embeddings.append([protein_id] + cnn_emb.tolist())\n",
    "\n",
    "# Convert to DataFrames\n",
    "protein_embeddings_df = pd.DataFrame(protein_embeddings, columns=['UniProt_ID'] + [f\"embedding_dim_{i+1}\" for i in range(protT5_emb.shape[0])])\n",
    "cnn_embeddings_df = pd.DataFrame(cnn_embeddings, columns=['UniProt_ID'] + [f\"cnn_dim_{i+1}\" for i in range(cnn_emb.shape[0])])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "protein_embeddings_df.iloc[:, 1:] = scaler.fit_transform(protein_embeddings_df.iloc[:, 1:])\n",
    "cnn_embeddings_df.iloc[:, 1:] = scaler.fit_transform(cnn_embeddings_df.iloc[:, 1:])\n",
    "\n",
    "# Save feature files\n",
    "protein_embeddings_df.to_csv(protT5_output, index=False)\n",
    "cnn_embeddings_df.to_csv(cnn_output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7784f16-826f-4425-9be8-746ca08573f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_chemberta_file = os.path.join(working_dir, \"ChemBERTa_drug_embeddings.csv\")\n",
    "drug_morgan_file = os.path.join(working_dir, \"Morgan_fingerprints.csv\")\n",
    "protein_prott5_file = os.path.join(working_dir, \"ProtT5_protein_embeddings.csv\")\n",
    "protein_cnn_file = os.path.join(working_dir, \"CNN_protein_embeddings.csv\")\n",
    "link_file = os.path.join(working_dir, \"Link.csv\")\n",
    "\n",
    "train_output = os.path.join(working_dir, \"train.csv\")\n",
    "val_output = os.path.join(working_dir, \"val.csv\")\n",
    "test_output = os.path.join(working_dir, \"test.csv\")\n",
    "\n",
    "# Load embeddings\n",
    "drug_chemberta_df = pd.read_csv(drug_chemberta_file)\n",
    "drug_morgan_df = pd.read_csv(drug_morgan_file)\n",
    "protein_prott5_df = pd.read_csv(protein_prott5_file)\n",
    "protein_cnn_df = pd.read_csv(protein_cnn_file)\n",
    "link_df = pd.read_csv(link_file)\n",
    "\n",
    "# Merge drug embeddings\n",
    "drug_embeddings = drug_chemberta_df.merge(drug_morgan_df, on='DrugBank_ID', how='inner')\n",
    "\n",
    "# Merge protein embeddings\n",
    "protein_embeddings = protein_prott5_df.merge(protein_cnn_df, on='UniProt_ID', how='inner')\n",
    "\n",
    "# Merge with drug-protein interactions\n",
    "dataset = link_df.merge(drug_embeddings, on='DrugBank_ID', how='inner')\n",
    "dataset = dataset.merge(protein_embeddings, on='UniProt_ID', how='inner')\n",
    "\n",
    "# Apply log-normalization on Kd values\n",
    "dataset['Kd(nM)'] = np.log10(dataset['Kd(nM)'] + 1)\n",
    "\n",
    "# Extract feature columns\n",
    "features = dataset.drop(columns=['DrugBank_ID', 'UniProt_ID', 'Kd(nM)'])\n",
    "target = dataset['Kd(nM)']\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "# Ensure minimum bin size for stratified splitting\n",
    "num_bins = min(10, len(np.unique(target)))\n",
    "while num_bins > 1:\n",
    "    discretizer = KBinsDiscretizer(n_bins=num_bins, encode='ordinal', strategy='quantile')\n",
    "    binned_target = discretizer.fit_transform(target.values.reshape(-1, 1)).flatten()\n",
    "    bin_counts = np.bincount(binned_target.astype(int))\n",
    "    if np.min(bin_counts) >= 5:\n",
    "        stratify_param = binned_target\n",
    "        break\n",
    "    else:\n",
    "        num_bins -= 1\n",
    "\n",
    "if num_bins == 1:\n",
    "    stratify_param = None\n",
    "\n",
    "# Implement Multi-Head Attention for Feature Fusion\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output.squeeze(1)\n",
    "\n",
    "# Convert features to PyTorch tensor\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "# Apply Multi-Head Attention Fusion\n",
    "attention_layer = MultiHeadAttention(input_dim=features.shape[1], num_heads=8)\n",
    "features_fused = attention_layer(features_tensor).detach().numpy()\n",
    "\n",
    "# Split into train and test sets\n",
    "train_features, test_features, train_target, test_target = train_test_split(\n",
    "    features_fused, target, test_size=0.2, stratify=stratify_param, random_state=42\n",
    ")\n",
    "\n",
    "# Re-bin test set to avoid single-member classes in stratification\n",
    "num_bins_test = min(10, len(np.unique(test_target)))\n",
    "while num_bins_test > 1:\n",
    "    discretizer_test = KBinsDiscretizer(n_bins=num_bins_test, encode='ordinal', strategy='quantile')\n",
    "    binned_test_target = discretizer_test.fit_transform(test_target.values.reshape(-1, 1)).flatten()\n",
    "    bin_counts_test = np.bincount(binned_test_target.astype(int))\n",
    "    if np.min(bin_counts_test) >= 2:\n",
    "        stratify_test = binned_test_target\n",
    "        break\n",
    "    else:\n",
    "        num_bins_test -= 1\n",
    "\n",
    "if num_bins_test == 1:\n",
    "    stratify_test = None\n",
    "\n",
    "# Split into validation and test sets\n",
    "val_features, test_features, val_target, test_target = train_test_split(\n",
    "    test_features, test_target, test_size=0.5, stratify=stratify_test, random_state=42\n",
    ")\n",
    "\n",
    "# Save datasets\n",
    "train_df = pd.DataFrame(train_features)\n",
    "train_df['Kd(nM)'] = train_target.values\n",
    "train_df.to_csv(train_output, index=False)\n",
    "\n",
    "val_df = pd.DataFrame(val_features)\n",
    "val_df['Kd(nM)'] = val_target.values\n",
    "val_df.to_csv(val_output, index=False)\n",
    "\n",
    "test_df = pd.DataFrame(test_features)\n",
    "test_df['Kd(nM)'] = test_target.values\n",
    "test_df.to_csv(test_output, index=False)\n",
    "\n",
    "print(\"Final datasets saved successfully: train.csv, val.csv, test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5bc411-b608-4cf5-9164-86580e2967b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# 1. Global Config & Paths\n",
    "############################################\n",
    "train_file    = os.path.join(working_dir, \"train.csv\")\n",
    "val_file      = os.path.join(working_dir, \"val.csv\")\n",
    "test_file     = os.path.join(working_dir, \"test.csv\")\n",
    "\n",
    "# Dimensions (for your data: 2816 + 1152 = 3968)\n",
    "drug_dim      = 2816  \n",
    "prot_dim      = 1152  \n",
    "\n",
    "# Hyperparams\n",
    "batch_size    = 256\n",
    "num_epochs    = 60\n",
    "patience      = 8\n",
    "learning_rate = 1e-4\n",
    "l2_reg        = 1e-5\n",
    "dropout_rate  = 0.2\n",
    "\n",
    "# Ensemble\n",
    "ensemble_size = 3\n",
    "\n",
    "# Output\n",
    "best_model_basename = \"best_model\"\n",
    "final_ensemble_file = os.path.join(working_dir, \"ensemble_preds.npy\")\n",
    "final_plots_prefix  = \"ensemble\"\n",
    "\n",
    "############################################\n",
    "# 2. Cosine Decay Callback\n",
    "############################################\n",
    "class CosineDecayRestarts(Callback):\n",
    "    \"\"\"\n",
    "    Cosine decay with restarts for the learning rate.\n",
    "    - T_max: number of epochs per cycle\n",
    "    - restart_factor: factor to expand T_max after each cycle\n",
    "    \"\"\"\n",
    "    def __init__(self, initial_lr=1e-4, min_lr=1e-6, T_max=10, restart_factor=1.5):\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.T_max = T_max\n",
    "        self.restart_factor = restart_factor\n",
    "        self.epoch_since_restart = 0\n",
    "        self.cycle_count = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        cycle_progress = (self.epoch_since_restart % self.T_max) / float(self.T_max)\n",
    "        cos_inner = np.pi * cycle_progress\n",
    "        cos_out = np.cos(cos_inner) + 1\n",
    "        new_lr = self.min_lr + 0.5 * (self.initial_lr - self.min_lr) * cos_out\n",
    "\n",
    "        if hasattr(self.model.optimizer, \"learning_rate\") and isinstance(self.model.optimizer.learning_rate, tf.Variable):\n",
    "            tf.keras.backend.set_value(self.model.optimizer.learning_rate, new_lr)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_since_restart += 1\n",
    "        if self.epoch_since_restart >= self.T_max:\n",
    "            self.epoch_since_restart = 0\n",
    "            self.cycle_count += 1\n",
    "            self.T_max = int(self.T_max * self.restart_factor)\n",
    "\n",
    "############################################\n",
    "# 3. Data Loading\n",
    "############################################\n",
    "def load_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    expected_cols = drug_dim + prot_dim + 1\n",
    "    if df.shape[1] != expected_cols:\n",
    "        raise ValueError(f\"{csv_path} has {df.shape[1]} columns, expected {expected_cols}.\")\n",
    "    if \"Kd(nM)\" not in df.columns:\n",
    "        raise ValueError(f\"Column 'Kd(nM)' not found in {csv_path}.\")\n",
    "    \n",
    "    X_drug = df.iloc[:, :drug_dim].values\n",
    "    X_prot = df.iloc[:, drug_dim:drug_dim+prot_dim].values\n",
    "    y      = df[\"Kd(nM)\"].values.astype(np.float32)\n",
    "    return X_drug, X_prot, y\n",
    "\n",
    "print(\"Loading train data...\")\n",
    "X_drug_train, X_prot_train, y_train = load_dataset(train_file)\n",
    "print(\"Loading validation data...\")\n",
    "X_drug_val, X_prot_val, y_val = load_dataset(val_file)\n",
    "print(\"Loading test data...\")\n",
    "X_drug_test, X_prot_test, y_test = load_dataset(test_file)\n",
    "\n",
    "############################################\n",
    "# 4. Build Model\n",
    "############################################\n",
    "def build_model(\n",
    "    drug_dim, \n",
    "    prot_dim, \n",
    "    hidden_drug=[1024, 512, 256],\n",
    "    hidden_prot=[1024, 512, 256],\n",
    "    hidden_merged=[1024, 512, 128, 64],\n",
    "    dropout=0.2,\n",
    "    l2_rate=1e-5,\n",
    "    lr=1e-4\n",
    "):\n",
    "    reg = regularizers.l2(l2_rate)\n",
    "\n",
    "    # Drug sub-network\n",
    "    drug_input = Input(shape=(drug_dim,), name=\"drug_input\")\n",
    "    xA = drug_input\n",
    "    for size in hidden_drug:\n",
    "        xA = layers.Dense(size, kernel_regularizer=reg)(xA)\n",
    "        xA = layers.BatchNormalization()(xA)\n",
    "        xA = layers.ReLU()(xA)\n",
    "        xA = layers.Dropout(dropout)(xA)\n",
    "\n",
    "    # Protein sub-network\n",
    "    prot_input = Input(shape=(prot_dim,), name=\"protein_input\")\n",
    "    xB = prot_input\n",
    "    for size in hidden_prot:\n",
    "        xB = layers.Dense(size, kernel_regularizer=reg)(xB)\n",
    "        xB = layers.BatchNormalization()(xB)\n",
    "        xB = layers.ReLU()(xB)\n",
    "        xB = layers.Dropout(dropout)(xB)\n",
    "\n",
    "    # Merge sub-networks\n",
    "    merged = layers.Concatenate(axis=1)([xA, xB])\n",
    "\n",
    "    # Merged branch with a residual block\n",
    "    x = layers.Dense(1024, kernel_regularizer=reg)(merged)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    # Residual block (keeping dimensions at 1024)\n",
    "    y = layers.Dense(1024, kernel_regularizer=reg)(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.LeakyReLU()(y)\n",
    "    y = layers.Dropout(0.1)(y)\n",
    "    x = layers.add([x, y])\n",
    "\n",
    "    # Further layers to reduce dimensions\n",
    "    for size in [512, 128, 64]:\n",
    "        x = layers.Dense(size, kernel_regularizer=reg)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    # Output layer\n",
    "    output = layers.Dense(1, activation='linear', name=\"Kd_output\")(x)\n",
    "\n",
    "    model = Model(inputs=[drug_input, prot_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss=Huber(delta=1.0),\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "############################################\n",
    "# 5. Training a Single Model\n",
    "############################################\n",
    "def train_single_model(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    model = build_model(\n",
    "        drug_dim=drug_dim,\n",
    "        prot_dim=prot_dim,\n",
    "        dropout=dropout_rate,\n",
    "        l2_rate=l2_reg,\n",
    "        lr=learning_rate\n",
    "    )\n",
    "\n",
    "    cos_callback = CosineDecayRestarts(\n",
    "        initial_lr=learning_rate,\n",
    "        min_lr=1e-6,\n",
    "        T_max=10,\n",
    "        restart_factor=1.5\n",
    "    )\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    model_ckpt_path = os.path.join(working_dir, f\"{best_model_basename}_seed{seed}.keras\")\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        model_ckpt_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x={\"drug_input\": X_drug_train, \"protein_input\": X_prot_train},\n",
    "        y=y_train,\n",
    "        validation_data=(\n",
    "            {\"drug_input\": X_drug_val, \"protein_input\": X_prot_val}, \n",
    "            y_val\n",
    "        ),\n",
    "        epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[cos_callback, early_stopping, checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.load_weights(model_ckpt_path)\n",
    "    return model, history\n",
    "\n",
    "############################################\n",
    "# 6. Ensemble Training\n",
    "############################################\n",
    "all_histories = []\n",
    "models = []\n",
    "\n",
    "for seed_idx in range(ensemble_size):\n",
    "    print(f\"\\n=== Training Ensemble Model #{seed_idx+1}/{ensemble_size} (seed={seed_idx}) ===\")\n",
    "    trained_model, history = train_single_model(seed=seed_idx)\n",
    "    all_histories.append(history)\n",
    "    models.append(trained_model)\n",
    "\n",
    "############################################\n",
    "# 7. Ensemble Prediction on Test Set\n",
    "############################################\n",
    "print(\"\\n=== Ensemble Prediction on Test Set ===\")\n",
    "pred_list = []\n",
    "for m in models:\n",
    "    preds = m.predict({\"drug_input\": X_drug_test, \"protein_input\": X_prot_test}).flatten()\n",
    "    pred_list.append(preds)\n",
    "\n",
    "ensemble_preds = np.mean(pred_list, axis=0)\n",
    "np.save(final_ensemble_file, ensemble_preds)\n",
    "\n",
    "mse  = mean_squared_error(y_test, ensemble_preds)\n",
    "mae  = mean_absolute_error(y_test, ensemble_preds)\n",
    "rmse = np.sqrt(mse)\n",
    "r2   = r2_score(y_test, ensemble_preds)\n",
    "\n",
    "print(\"\\n=== Final Ensemble Test Results ===\")\n",
    "print(f\"MSE:  {mse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "\n",
    "############################################\n",
    "# 8. Plotting\n",
    "############################################\n",
    "max_epochs = max(len(h.history[\"loss\"]) for h in all_histories)\n",
    "avg_train_loss = np.zeros(max_epochs)\n",
    "avg_val_loss   = np.zeros(max_epochs)\n",
    "count_epochs   = np.zeros(max_epochs)\n",
    "\n",
    "for hist in all_histories:\n",
    "    train_l = hist.history[\"loss\"]\n",
    "    val_l   = hist.history[\"val_loss\"]\n",
    "    for e in range(len(train_l)):\n",
    "        avg_train_loss[e] += train_l[e]\n",
    "        avg_val_loss[e]   += val_l[e]\n",
    "        count_epochs[e]   += 1\n",
    "\n",
    "for e in range(max_epochs):\n",
    "    if count_epochs[e] > 0:\n",
    "        avg_train_loss[e] /= count_epochs[e]\n",
    "        avg_val_loss[e]   /= count_epochs[e]\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "epochs_range = range(1, max_epochs+1)\n",
    "plt.plot(epochs_range, avg_train_loss[:max_epochs], label=\"Train Loss (avg)\", linewidth=2)\n",
    "plt.plot(epochs_range, avg_val_loss[:max_epochs],   label=\"Val Loss (avg)\", linewidth=2)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Huber Loss\")\n",
    "plt.title(\"Ensemble: Training vs Validation Loss (Averaged)\")\n",
    "plt.legend()\n",
    "loss_curve_path = os.path.join(working_dir, f\"{final_plots_prefix}_loss_curve.png\")\n",
    "plt.savefig(loss_curve_path, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, ensemble_preds, alpha=0.5)\n",
    "plt.xlabel(\"Actual Kd(nM)\")\n",
    "plt.ylabel(\"Predicted Kd(nM)\")\n",
    "plt.title(\"Ensemble: Predicted vs Actual Kd\")\n",
    "pred_vs_act_path = os.path.join(working_dir, f\"{final_plots_prefix}_predicted_vs_actual.png\")\n",
    "plt.savefig(pred_vs_act_path, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "residuals = y_test - ensemble_preds\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(residuals, bins=30, alpha=0.7)\n",
    "plt.xlabel(\"Residual (Actual - Predicted)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Ensemble: Residual Distribution\")\n",
    "residual_plot_path = os.path.join(working_dir, f\"{final_plots_prefix}_residual_plot.png\")\n",
    "plt.savefig(residual_plot_path, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll ensemble plots saved:\")\n",
    "print(loss_curve_path)\n",
    "print(pred_vs_act_path)\n",
    "print(residual_plot_path)\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af693ef3-5ae1-4ec3-bd71-403567ab0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# 1. Configuration\n",
    "######################################################\n",
    "# Input CSVs\n",
    "new_drugs_file    = os.path.join(working_dir, \"New_drugs.csv\")\n",
    "new_protein_file  = os.path.join(working_dir, \"New_Proteins.csv\")\n",
    "\n",
    "# auto-generate all pairs\n",
    "new_pairs_file    = os.path.join(working_dir, \"New_Pairs.csv\")\n",
    "\n",
    "# Output for intermediate embeddings\n",
    "chemberta_drug_csv  = os.path.join(working_dir, \"ChemBERTa_new_drug_embeddings.csv\")\n",
    "morgan_drug_csv     = os.path.join(working_dir, \"Morgan_new_fingerprints.csv\")\n",
    "protT5_csv          = os.path.join(working_dir, \"ProtT5_new_protein_embeddings.csv\")\n",
    "cnn_csv             = os.path.join(working_dir, \"CNN_new_protein_embeddings.csv\")\n",
    "\n",
    "# Final fused features after multi-head attention\n",
    "final_fused_csv     = os.path.join(working_dir, \"New_Features_Fused.csv\")\n",
    "\n",
    "# Drug/protein dimension assumptions after merging\n",
    "drug_dim = 2816\n",
    "prot_dim = 1152\n",
    "\n",
    "######################################################\n",
    "# 2. Step A: Create New_Pairs.csv (Cartesian product)\n",
    "######################################################\n",
    "print(\"=== Creating all possible drug–protein pairs ===\")\n",
    "df_drug = pd.read_csv(new_drugs_file)     # expects columns: [DrugBank_ID, ...]\n",
    "df_prot = pd.read_csv(new_protein_file)   # expects columns: [UniProt_ID, ...]\n",
    "\n",
    "pairs_list = []\n",
    "for d_id in df_drug[\"DrugBank_ID\"]:\n",
    "    for p_id in df_prot[\"UniProt_ID\"]:\n",
    "        pairs_list.append([d_id, p_id])\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs_list, columns=[\"DrugBank_ID\", \"UniProt_ID\"])\n",
    "pairs_df.to_csv(new_pairs_file, index=False)\n",
    "print(f\"New_Pairs.csv created with {len(pairs_df)} rows.\")\n",
    "\n",
    "######################################################\n",
    "# 3. Step B: Extract Drug Embeddings\n",
    "######################################################\n",
    "print(\"=== Extracting drug embeddings (ChemBERTa + Morgan) ===\")\n",
    "# Force device to CPU explicitly for PyTorch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer_chem = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model_chem     = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\").to(device)\n",
    "\n",
    "def tokenize_smiles(smiles, max_len=512, stride=256):\n",
    "    tokens = tokenizer_chem(smiles, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    if tokens[\"input_ids\"].shape[1] > max_len:\n",
    "        input_ids = tokens[\"input_ids\"]\n",
    "        chunks = []\n",
    "        for i in range(0, input_ids.shape[1], stride):\n",
    "            chunks.append(input_ids[:, i:i+max_len])\n",
    "        return chunks\n",
    "    return [tokens[\"input_ids\"]]\n",
    "\n",
    "def get_chemberta_embedding(smiles):\n",
    "    chunk_list = tokenize_smiles(smiles)\n",
    "    emb_list = []\n",
    "    for chunk in chunk_list:\n",
    "        chunk = chunk.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model_chem(input_ids=chunk)\n",
    "        emb = out.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        emb_list.append(emb)\n",
    "    return np.mean(np.vstack(emb_list), axis=0)\n",
    "\n",
    "def get_morgan_fingerprint(smiles, radius=2, n_bits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.zeros(n_bits)\n",
    "    return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits))\n",
    "\n",
    "drugs_df = pd.read_csv(new_drugs_file)\n",
    "chemberta_list = []\n",
    "morgan_list    = []\n",
    "\n",
    "for _, row in tqdm(drugs_df.iterrows(), total=len(drugs_df)):\n",
    "    d_id   = row[\"DrugBank_ID\"]\n",
    "    smiles = row[\"Ligand_SMILES\"]\n",
    "\n",
    "    chem_emb = get_chemberta_embedding(smiles)\n",
    "    morgan   = get_morgan_fingerprint(smiles)\n",
    "\n",
    "    chemberta_list.append([d_id] + chem_emb.tolist())\n",
    "    morgan_list.append([d_id] + morgan.tolist())\n",
    "\n",
    "chem_dim    = len(chemberta_list[0]) - 1\n",
    "morgan_dim  = len(morgan_list[0])    - 1\n",
    "\n",
    "chem_cols    = [\"DrugBank_ID\"] + [f\"chem_{i}\"   for i in range(chem_dim)]\n",
    "morgan_cols  = [\"DrugBank_ID\"] + [f\"morgan_{i}\" for i in range(morgan_dim)]\n",
    "\n",
    "chem_df   = pd.DataFrame(chemberta_list, columns=chem_cols)\n",
    "morgan_df = pd.DataFrame(morgan_list,    columns=morgan_cols)\n",
    "\n",
    "# Scale each\n",
    "scalerA = StandardScaler()\n",
    "chem_df.iloc[:, 1:] = scalerA.fit_transform(chem_df.iloc[:, 1:])\n",
    "\n",
    "scalerB = StandardScaler()\n",
    "morgan_df.iloc[:, 1:] = scalerB.fit_transform(morgan_df.iloc[:, 1:])\n",
    "\n",
    "chem_df.to_csv(chemberta_drug_csv, index=False)\n",
    "morgan_df.to_csv(morgan_drug_csv,   index=False)\n",
    "\n",
    "######################################################\n",
    "# 4. Step C: Extract Protein Embeddings\n",
    "######################################################\n",
    "print(\"=== Extracting protein embeddings (ProtT5 + CNN) ===\")\n",
    "tokenizer_t5 = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n",
    "model_t5     = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\").to(device)\n",
    "\n",
    "def tokenize_protein(seq, max_len=512, stride=256):\n",
    "    tokens = tokenizer_t5(seq, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len)\n",
    "    if tokens[\"input_ids\"].shape[1] > max_len:\n",
    "        input_ids = tokens[\"input_ids\"]\n",
    "        chunks = []\n",
    "        for i in range(0, input_ids.shape[1], stride):\n",
    "            chunks.append(input_ids[:, i:i+max_len])\n",
    "        return chunks\n",
    "    return [tokens[\"input_ids\"]]\n",
    "\n",
    "def get_protT5_embedding(seq):\n",
    "    chunk_list = tokenize_protein(seq)\n",
    "    emb_list = []\n",
    "    for chunk in chunk_list:\n",
    "        chunk = chunk.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model_t5(input_ids=chunk)\n",
    "        emb = out.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        emb_list.append(emb)\n",
    "    return np.mean(np.vstack(emb_list), axis=0)\n",
    "\n",
    "# CNN for ASCII\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(64, kernel_size=3, activation='relu', input_shape=(1000, 1)),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(128, activation='relu')\n",
    "])\n",
    "\n",
    "def get_cnn_embedding(seq):\n",
    "    arr = np.array([ord(a) for a in seq[:1000]], dtype=np.float32)\n",
    "    if len(arr) < 1000:\n",
    "        arr = np.pad(arr, (0, 1000 - len(arr)), 'constant', constant_values=0)\n",
    "    arr = arr.reshape(1, 1000, 1)\n",
    "    emb = cnn_model.predict(arr, verbose=0)[0]\n",
    "    return emb\n",
    "\n",
    "prot_df = pd.read_csv(new_protein_file)\n",
    "t5_list  = []\n",
    "cnn_list = []\n",
    "\n",
    "for _, row in tqdm(prot_df.iterrows(), total=len(prot_df)):\n",
    "    p_id  = row[\"UniProt_ID\"]\n",
    "    seq   = row[\"Sequence\"]\n",
    "\n",
    "    t5_emb  = get_protT5_embedding(seq)\n",
    "    c_emb   = get_cnn_embedding(seq)\n",
    "\n",
    "    t5_list.append([p_id] + t5_emb.tolist())\n",
    "    cnn_list.append([p_id] + c_emb.tolist())\n",
    "\n",
    "t5_dim   = len(t5_list[0])  - 1\n",
    "cnn_dim  = len(cnn_list[0]) - 1\n",
    "\n",
    "t5_cols  = [\"UniProt_ID\"] + [f\"t5_{i}\"  for i in range(t5_dim)]\n",
    "cnn_cols = [\"UniProt_ID\"] + [f\"cnn_{i}\" for i in range(cnn_dim)]\n",
    "\n",
    "t5_new_df  = pd.DataFrame(t5_list,  columns=t5_cols)\n",
    "cnn_new_df = pd.DataFrame(cnn_list, columns=cnn_cols)\n",
    "\n",
    "scalerC = StandardScaler()\n",
    "t5_new_df.iloc[:, 1:]  = scalerC.fit_transform(t5_new_df.iloc[:, 1:])\n",
    "\n",
    "scalerD = StandardScaler()\n",
    "cnn_new_df.iloc[:, 1:] = scalerD.fit_transform(cnn_new_df.iloc[:, 1:])\n",
    "\n",
    "t5_new_df.to_csv(protT5_csv, index=False)\n",
    "cnn_new_df.to_csv(cnn_csv,   index=False)\n",
    "\n",
    "######################################################\n",
    "# 5. Step D: Merge All + Create Final Feature Matrix\n",
    "######################################################\n",
    "print(\"=== Merging drug + protein embeddings with New_Pairs.csv ===\")\n",
    "pairs_df     = pd.read_csv(new_pairs_file)\n",
    "drug_chem_df = pd.read_csv(chemberta_drug_csv)\n",
    "drug_morg_df = pd.read_csv(morgan_drug_csv)\n",
    "\n",
    "prot_t5_df  = pd.read_csv(protT5_csv)\n",
    "prot_cnn_df = pd.read_csv(cnn_csv)\n",
    "\n",
    "# Merge drug embeddings => shape (Ndrugs, 1 + dims)\n",
    "drug_all = drug_chem_df.merge(drug_morg_df, on=\"DrugBank_ID\", how=\"inner\")\n",
    "# Merge protein embeddings => shape (Nprots, 1 + dims)\n",
    "prot_all = prot_t5_df.merge(prot_cnn_df, on=\"UniProt_ID\", how=\"inner\")\n",
    "\n",
    "# Now merge with pairs => final shape: [DrugBank_ID, UniProt_ID, ~2816 drug cols, ~1152 prot cols]\n",
    "merge1 = pairs_df.merge(drug_all, on=\"DrugBank_ID\", how=\"inner\")\n",
    "merge2 = merge1.merge(prot_all, on=\"UniProt_ID\", how=\"inner\")\n",
    "print(\"Final merged shape:\", merge2.shape)\n",
    "\n",
    "# Feature matrix\n",
    "feat_cols = [c for c in merge2.columns if c not in [\"DrugBank_ID\", \"UniProt_ID\"]]\n",
    "X_merged  = merge2[feat_cols].values  # shape (N, 3968)\n",
    "\n",
    "######################################################\n",
    "# 6. Step E: Multi-Head Attention Fusion\n",
    "######################################################\n",
    "print(\"=== Applying Multi-Head Attention across columns ===\")\n",
    "\n",
    "class MyMHA(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=8):\n",
    "        super(MyMHA, self).__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=input_dim, num_heads=num_heads, batch_first=True)\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, input_dim)\n",
    "        x = x.unsqueeze(1)  # (batch, 1, input_dim)\n",
    "        out, _ = self.mha(x, x, x)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "attn_layer = MyMHA(input_dim=X_merged.shape[1], num_heads=8)\n",
    "with torch.no_grad():\n",
    "    fused_tensor = torch.tensor(X_merged, dtype=torch.float32)\n",
    "    fused_out    = attn_layer(fused_tensor).numpy()\n",
    "\n",
    "print(\"Fused feature shape:\", fused_out.shape)\n",
    "\n",
    "# Save final to CSV\n",
    "fused_cols = [f\"feature_{i}\" for i in range(fused_out.shape[1])]\n",
    "fused_df   = pd.DataFrame(fused_out, columns=fused_cols)\n",
    "# Keep IDs for reference\n",
    "fused_df[\"DrugBank_ID\"] = merge2[\"DrugBank_ID\"].values\n",
    "fused_df[\"UniProt_ID\"]  = merge2[\"UniProt_ID\"].values\n",
    "\n",
    "fused_df.to_csv(final_fused_csv, index=False)\n",
    "print(f\"Saved final fused features to {final_fused_csv}\")\n",
    "print(\"Feature extraction + attention done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1da623-c791-470a-9740-7d1d6f63bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# 1. Configuration\n",
    "############################################################\n",
    "\n",
    "# Fused features from the first script\n",
    "final_fused_csv = os.path.join(working_dir, \"New_Features_Fused.csv\")\n",
    "\n",
    "# Ensemble of 3 seeds\n",
    "ensemble_models = [\n",
    "    os.path.join(working_dir, \"best_model_seed0.keras\"),\n",
    "    os.path.join(working_dir, \"best_model_seed1.keras\"),\n",
    "    os.path.join(working_dir, \"best_model_seed2.keras\"),\n",
    "]\n",
    "\n",
    "# Final outputs\n",
    "pred_csv        = os.path.join(working_dir, \"New_Pairs_PredictedKd.csv\")\n",
    "top20_csv       = os.path.join(working_dir, \"Top20_LowestKd.csv\")\n",
    "hist_plot       = os.path.join(working_dir, \"Kd_Distribution_NewData.png\")\n",
    "bar_plot        = os.path.join(working_dir, \"Top20_NewData.png\")\n",
    "\n",
    "# Drug/protein dimensions (these must match what the model expects)\n",
    "drug_dim = 2816\n",
    "prot_dim = 1152\n",
    "\n",
    "############################################################\n",
    "# 2. Load Fused Features\n",
    "############################################################\n",
    "fused_df = pd.read_csv(final_fused_csv)\n",
    "feat_cols = [c for c in fused_df.columns if c not in [\"DrugBank_ID\", \"UniProt_ID\"]]\n",
    "X_fused = fused_df[feat_cols].values\n",
    "\n",
    "# Keep IDs for reference\n",
    "id_drug = fused_df[\"DrugBank_ID\"].values\n",
    "id_prot = fused_df[\"UniProt_ID\"].values\n",
    "\n",
    "print(\"Loaded fused features shape:\", X_fused.shape)\n",
    "\n",
    "# Split into drug and protein inputs for the model\n",
    "X_drug_fused = X_fused[:, :drug_dim]   # First 2816 columns → Drug features\n",
    "X_prot_fused = X_fused[:, drug_dim:]   # Last 1152 columns → Protein features\n",
    "\n",
    "############################################################\n",
    "# 3. Ensemble Prediction\n",
    "############################################################\n",
    "pred_list = []\n",
    "for model_path in ensemble_models:\n",
    "    if not os.path.isfile(model_path):\n",
    "        print(f\"Warning: {model_path} not found. Skipping!\")\n",
    "        continue\n",
    "    print(f\"Loading {model_path}\")\n",
    "    model = load_model(model_path, compile=False)\n",
    "\n",
    "    # Pass them as two separate inputs\n",
    "    preds = model.predict({\"drug_input\": X_drug_fused, \"protein_input\": X_prot_fused}, verbose=0).flatten()\n",
    "    pred_list.append(preds)\n",
    "\n",
    "if len(pred_list) == 0:\n",
    "    print(\"No models loaded; exiting.\")\n",
    "    exit()\n",
    "\n",
    "ensemble_preds = np.mean(pred_list, axis=0)\n",
    "\n",
    "# If you trained with log10(Kd+1), invert:\n",
    "kd_preds = 10**ensemble_preds - 1\n",
    "\n",
    "############################################################\n",
    "# 4. Save Full Predictions\n",
    "############################################################\n",
    "results_df = pd.DataFrame({\n",
    "    \"DrugBank_ID\": id_drug,\n",
    "    \"UniProt_ID\":  id_prot,\n",
    "    \"Predicted_Kd\": kd_preds\n",
    "})\n",
    "results_df.to_csv(pred_csv, index=False)\n",
    "print(f\"Saved predictions to {pred_csv}\")\n",
    "\n",
    "############################################################\n",
    "# 5. Top 20 + Plots\n",
    "############################################################\n",
    "top20_df = results_df.sort_values(\"Predicted_Kd\", ascending=True).head(20)\n",
    "top20_df.to_csv(top20_csv, index=False)\n",
    "print(f\"Saved top-20 to {top20_csv}\")\n",
    "\n",
    "# Distribution plot\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(kd_preds, bins=30, alpha=0.7)\n",
    "plt.xlabel(\"Predicted Kd (nM)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Predicted Kd Distribution (New Data)\")\n",
    "plt.savefig(hist_plot, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "# Top 20 bar chart\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(np.arange(len(top20_df)), top20_df[\"Predicted_Kd\"], align=\"center\")\n",
    "labels = [f\"{r.DrugBank_ID}-{r.UniProt_ID}\" for _, r in top20_df.iterrows()]\n",
    "plt.yticks(np.arange(len(top20_df)), labels)\n",
    "plt.xlabel(\"Predicted Kd (nM)\")\n",
    "plt.title(\"Top 20 Lowest Predicted Kd (New Data)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig(bar_plot, dpi=1200)\n",
    "plt.show()\n",
    "\n",
    "print(\"Plots saved:\")\n",
    "print(\" \", hist_plot)\n",
    "print(\" \", bar_plot)\n",
    "print(\"\\nPrediction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff87ce-fb56-4632-9502-63b0d1013565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a53b19-0dcb-4ea8-aa1e-8e3369943598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
